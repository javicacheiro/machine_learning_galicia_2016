{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS WITH SPARK ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Main Concepts\n",
    "\n",
    "- **DataFrame**: a table with built-in operations\n",
    "\n",
    "- **Transformer**: transforms one DataFrame into another DataFrame\n",
    "\n",
    "- **Estimator**: eg. a learning algorithm that trains on a DataFrame and produces a Model\n",
    "\n",
    "- **Pipeline**: chains Transformers and Estimators to produce a Model\n",
    "\n",
    "- **Evaluator**: measures how well a fitted Model does on held-out test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon product data\n",
    "We will use a [dataset](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz)[1] that contains 8.9M book reviews from Amazon, spanning May 1996 - July 2014.\n",
    "\n",
    "Dataset characteristics:\n",
    "- Number of reviews: 8.9M\n",
    "- Size: 8.8GB (uncompressed)\n",
    "- HDFS blocks: 70 (each with 3 replicas)\n",
    "\n",
    "\n",
    "[1] Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.26 ms, sys: 1.2 ms, total: 9.46 ms\n",
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "raw_reviews = sqlContext.read.json('data/amazon/reviews_Books_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|Spiritually and m...|    5.0|\n",
      "|This is one my mu...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 3.88 ms, sys: 998 Âµs, total: 4.88 ms\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_reviews = raw_reviews.select('reviewText', 'overall')\n",
    "all_reviews.cache()\n",
    "all_reviews.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "We will avoid neutral reviews by keeping only reviews with 1 or 5 stars overall score.\n",
    "We will also filter out the reviews that contain no text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonneutral_reviews = all_reviews.filter(\n",
    "    (all_reviews.overall == 1.0) | (all_reviews.overall == 5.0))\n",
    "reviews = nonneutral_reviews.filter(all_reviews.reviewText != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[reviewText: string, overall: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.cache()\n",
    "all_reviews.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData, testData = reviews.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Pipeline\n",
    "![pipeline](http://hadoop.cesga.es/files/sentiment_analysis/pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizer\n",
    "A transformer to convert numerical features to binary (0/1) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=2.5, inputCol='overall', outputCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "A transformer that converts the input string to lowercase and then splits it by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWordsRemover\n",
    "A transformer that filters out stop words from input. Note: null values from input array are preserved unless adding null to stopWords explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF\n",
    "A Transformer that converts a sequence of words into a fixed-length feature Vector. It maps a sequence of terms to their term frequencies using a hashing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remover' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-79ccf402414e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhashingTF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremover\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'remover' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[binarizer, tokenizer, remover, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 2.71 ms, total: 18.7 ms\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeLineModel = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/hdp/2.4.2.0-258/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC:  0.966076886439\n",
      "CPU times: user 2.34 s, sys: 1.34 s, total: 3.68 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "predictions = pipeLineModel.transform(testData)\n",
    "\n",
    "aur = evaluator.evaluate(predictions)\n",
    "\n",
    "print 'Area under ROC: ', aur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 10.8 s, total: 31.1 s\n",
      "Wall time: 12min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(hashingTF.numFeatures, [10000, 100000]) \\\n",
    "            .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "            .addGrid(lr.maxIter, [10, 20]) \\\n",
    "            .build()\n",
    "            \n",
    "cv = (CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(evaluator)\n",
    "      .setEstimatorParamMaps(param_grid)\n",
    "      .setNumFolds(3))\n",
    "\n",
    "cv_model = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC:  0.968664347321\n",
      "CPU times: user 942 ms, sys: 561 ms, total: 1.5 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_predictions = cv_model.transform(testData)\n",
    "new_aur = evaluator.evaluate(new_predictions)\n",
    "print 'Area under ROC: ', new_aur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
